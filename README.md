# Vertis Data Consultant ğŸ¤–ğŸ“Š

Plataforma em Streamlit para anÃ¡lise de dados de monitoramento do OpenShift. Foca em:
- IngestÃ£o eficiente de dados com DuckDB (Parquet/CSV)
- PrÃ©-processamento massivo com Dask (CSV â†’ Parquet particionado)
- Modelagem preditiva com scikit-learn, XGBoost e LightGBM
- DiagnÃ³sticos e interpretabilidade com SHAP
- UI interativa para exploraÃ§Ã£o e treino de modelos

Este README detalha a estrutura do repositÃ³rio, como configurar o ambiente, preparar os dados e executar a aplicaÃ§Ã£o.

---

## SumÃ¡rio

- VisÃ£o geral da arquitetura
- Estrutura do repositÃ³rio
- PrÃ©-requisitos
- ConfiguraÃ§Ã£o do ambiente
- Pipeline de dados (CSV â†’ Parquet)
- Executando a aplicaÃ§Ã£o (Streamlit)
- Uso: anÃ¡lise e modelagem
- DiagnÃ³sticos avanÃ§ados (Feature Importance e SHAP)
- Ajustes de desempenho
- SoluÃ§Ã£o de problemas
- ReferÃªncias de arquivos
- LicenÃ§a

---

## VisÃ£o geral da arquitetura

Fluxo principal:
1) CSV bruto â†’ [agent/preprocess.py](agent/preprocess.py) â†’ Parquet particionado por namespace
2) Parquets â†’ [`agent.data_loader.DataLoader`](agent/data_loader.py) â†’ Tabela logs (DuckDB)
3) Tabela logs â†’ PIVOT e features â†’ [`agent.predictive_model.PredictiveModel`](agent/predictive_model.py)
4) Interface Streamlit â†’ [app.py](app.py)

Componentes-chave:
- IngestÃ£o/consulta: [`agent.data_loader.DataLoader`](agent/data_loader.py)
- Modelagem: [`agent.predictive_model.PredictiveModel`](agent/predictive_model.py)
- UI Streamlit principal: [app.py](app.py)
- ConfiguraÃ§Ãµes (paths, modelos, alvos): [config.py](config.py)
- UtilitÃ¡rios/diagnÃ³sticos: [analysis.py](analysis.py), [utils.py](utils.py)

Obs.: IntegraÃ§Ã£o LLM/Ollama removida do fluxo (nÃ£o utilizada).

---

## Estrutura do repositÃ³rio

```
vertis_research_agent/
â”œâ”€â”€ app.py                      # AplicaÃ§Ã£o Streamlit principal
â”œâ”€â”€ analysis.py                 # FunÃ§Ãµes auxiliares de anÃ¡lise (opcional)
â”œâ”€â”€ ui_components.py            # Componentes/modos de UI (opcional)
â”œâ”€â”€ utils.py                    # UtilitÃ¡rios
â”œâ”€â”€ config.py                   # OpÃ§Ãµes de UI, mÃ©tricas e caminhos padrÃ£o
â”œâ”€â”€ check_duckdb.py             # (Opcional) verificaÃ§Ã£o/diagnÃ³stico do DuckDB
â”œâ”€â”€ requirements.txt            # DependÃªncias Python
â”œâ”€â”€ .env                        # VariÃ¡veis de ambiente (opcional; nÃ£o requer API)
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py          # DuckDB (CSV/Parquet â†’ tabela logs)
â”‚   â”œâ”€â”€ predictive_model.py     # PrÃ©-processamento + treino/avaliaÃ§Ã£o
â”‚   â””â”€â”€ preprocess.py           # Dask (CSV grande â†’ Parquet particionado)
â””â”€â”€ archive/
    â”œâ”€â”€ 00.csv                  # CSV bruto (exemplo)
    â””â”€â”€ partitioned_parquet/    # SaÃ­da Parquet particionada por ocnr_tx_namespace
```

---

## PrÃ©-requisitos

- Python 3.10+
- Linux: build-essential (para XGBoost/LightGBM), libomp (para LightGBM em algumas distros)
  - Debian/Ubuntu: sudo apt-get update && sudo apt-get install -y build-essential libgomp1
- Parquet: pyarrow (jÃ¡ em requirements)
- Recomendado: virtualenv/venv

Para diagnÃ³sticos avanÃ§ados:
- SHAP: pip install shap

---

## ConfiguraÃ§Ã£o do ambiente

1) Clonar o repositÃ³rio
```bash
git clone https://github.com/devgalvas/csv_agent.git
cd vertis-data-consultant/vertis_research_agent
```

2) (Recomendado) Criar ambiente virtual
```bash
python -m venv .venv
source .venv/bin/activate   # Linux/macOS
# .venv\Scripts\activate    # Windows (PowerShell)
```

3) Instalar dependÃªncias
```bash
pip install --upgrade pip
pip install -r requirements.txt
# Para os diagnÃ³sticos avanÃ§ados (se ainda nÃ£o incluso em requirements):
pip install shap
```

4) VariÃ¡veis de ambiente
- NÃ£o Ã© necessÃ¡rio configurar API keys. O arquivo `.env` Ã© opcional.

---

## Pipeline de dados (CSV â†’ Parquet)

Use o Dask para converter grandes CSVs para Parquet particionado por namespace:
```bash
python agent/preprocess.py
```

O script:
- LÃª o CSV de entrada (padrÃ£o: archive/00.csv)
- Normaliza tipos, remove nulos crÃ­ticos
- Salva particionado por `ocnr_tx_namespace` em `archive/partitioned_parquet`

Personalize parÃ¢metros (encoding, blocksize, colunas) diretamente em [agent/preprocess.py](agent/preprocess.py).

---

## Executando a aplicaÃ§Ã£o (Streamlit)

1) Garanta que existem Parquets em `archive/partitioned_parquet/`
2) Rode a aplicaÃ§Ã£o:
```bash
streamlit run app.py
```
3) No sidebar:
- Informe o diretÃ³rio Parquet (padrÃ£o em [config.py](config.py): `PARQUET_DEFAULT_PATH`)
- Selecione um Namespace (ou All)
- Clique em â€œğŸš€ Carregar Dados e Analisarâ€

A aplicaÃ§Ã£o criarÃ¡ a tabela `logs` no DuckDB usando [`DataLoader`](agent/data_loader.py).

---

## Uso: anÃ¡lise e modelagem

A UI de [app.py](app.py) disponibiliza:
- Carregamento e cache:
  - `load_data_and_get_schema`: conecta ao DuckDB e obtÃ©m schema
  - `get_namespaces_from_filesystem`: lista namespaces a partir do FS
  - `get_pivoted_dataframe_for_training`: executa PIVOT e retorna DataFrame â€œwideâ€ com mÃ©tricas por timestamp
- â€œğŸ¤– Modelagem Preditiva com Engenharia de Featuresâ€:
  - MÃ©trica-alvo (ex.: `NAMESPACE_CPU_USAGE`, `NAMESPACE_MEMORY_USAGE`, `NAMESPACE_POD_COUNT`)
  - Tipo de modelo (ver [config.py](config.py): `MODEL_OPTIONS`)
  - Tamanho de amostra para busca de dados
  - BotÃµes:
    - â€œTreinar Modeloâ€
    - â€œPlotar Performance BÃ¡sicaâ€
    - â€œPlotar DiagnÃ³sticos AvanÃ§adosâ€

Pipeline interno de modelagem ([predictive_model.py](agent/predictive_model.py)):
- OrdenaÃ§Ã£o temporal por `ocnr_dt_date`
- ConversÃ£o de alvo para numÃ©rico
- Lags: 1, 3, 5, 7
- Janelas rolantes (mean/std, janela=5)
- Features de tempo: hora, dia da semana, dia do mÃªs, mÃªs
- Alinhamento X/y, limpeza de NaNs, Ã­ndices temporais
- Treino com modelos lineares/Ã¡rvores; mÃ©tricas: MSE, RÂ² (e RMSE na UI estendida)

---

## DiagnÃ³sticos avanÃ§ados (Feature Importance e SHAP)

- ImportÃ¢ncia de features:
  - Modelos de Ã¡rvore: `feature_importances_` (barplot ordenado)
  - Modelos lineares: coeficientes (tabela ordenada)
- SHAP (para `random_forest`, `xgboost`, `lightgbm`):
  - `TreeExplainer` + `summary_plot (dot)` para impacto local/global
  - Ãštil para entender como cada mÃ©trica influencia as previsÃµes
- ObservaÃ§Ã£o: SHAP pode ser custoso em CPU/memÃ³ria; reduza o tamanho de amostra se necessÃ¡rio.

---

## Ajustes de desempenho

- PrÃ©-processamento:
  - Ajuste `blocksize` no Dask (p.ex. 64MB) para equilibrar memÃ³ria/velocidade
  - Parquet + particionamento por `ocnr_tx_namespace` acelera filtros
- Consultas:
  - Use LIMIT e um conjunto enxuto de mÃ©tricas no PIVOT
  - Prefira scans de Parquet a CSV em produÃ§Ã£o
- Modelos:
  - Ajuste `sample_size` no carregamento para controlar custo de treino
  - XGBoost/LightGBM: `n_jobs=-1` para paralelismo

---

## SoluÃ§Ã£o de problemas

- â€œDiretÃ³rio Parquet nÃ£o existeâ€
  - Corrija o caminho no sidebar (ou ajuste `PARQUET_DEFAULT_PATH` em [config.py](config.py))
  - Gere os Parquets: `python agent/preprocess.py`
- â€œFalha ao executar PIVOTâ€
  - Verifique colunas: `ocnr_dt_date`, `ocnr_tx_namespace`, `ocnr_tx_query`, `ocnr_nm_result`
  - Cheque se hÃ¡ dados no namespace selecionado
- â€œRÂ² negativo / erros altosâ€
  - Aumente a janela (mais linhas)
  - Revise a mÃ©trica-alvo e a sazonalidade
- â€œErro com SHAPâ€
  - Instale `shap` e confirme compatibilidade da versÃ£o do XGBoost/LightGBM
  - Reduza a amostra para o cÃ¡lculo SHAP

---

## ReferÃªncias de arquivos

- App Streamlit: [app.py](app.py)
  - Cache/ingestÃ£o/PIVOT: `load_data_and_get_schema`, `get_namespaces_from_filesystem`, `get_pivoted_dataframe_for_training`
- Data loader (DuckDB): [`agent.data_loader.DataLoader`](agent/data_loader.py)
- Modelagem: [`agent.predictive_model.PredictiveModel`](agent/predictive_model.py)
- ConfiguraÃ§Ãµes (paths/modelos/alvos): [config.py](config.py)
- Pipeline CSV â†’ Parquet: [agent/preprocess.py](agent/preprocess.py)

---

## LicenÃ§a